# data_pipeline_project

A data pipeline with Docker, Airflow, Kafka, Spark Streaming, cassandra

## Description

### Objective
The project is a streaming data pipeline. It covers each stage from data ingestion to processing and finally to storage, utilizing a robust tech stack that includes Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandra.Everything is containerized using Docker for ease of deployment and scalability.

### Architecture
![data_pipeline_project-architecture](images/System Architecture.jpg)

### Tools & Technologies
- Containerization - [**Docker**](https://www.docker.com), [**Docker Compose**](https://docs.docker.com/compose/)
- Stream Processing - [**Kafka**](https://kafka.apache.org), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- Orchestration - [**Airflow**](https://airflow.apache.org)
- Language - [**Python**](https://www.python.org)


