# data_pipeline_project

A data pipeline with Docker, Airflow, Kafka, Spark Streaming, cassandra

## Description

### Objective
The project is a streaming data pipeline. It covers each stage from data ingestion to processing and finally to storage, utilizing a robust tech stack that includes Apache Airflow, Python, Apache Kafka, Apache Zookeeper, Apache Spark, and Cassandra.Everything is containerized using Docker for ease of deployment and scalability.

### Architecture
![data_pipeline_project-architecture](https://github.com/mahesh-c-pathak/data_pipeline_project/blob/main/images/System%20Architecture.JPG)


### Tools & Technologies
- Containerization - [**Docker**](https://www.docker.com), [**Docker Compose**](https://docs.docker.com/compose/)
- Stream Processing - [**Kafka**](https://kafka.apache.org), [**Spark Streaming**](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
- Orchestration - [**Airflow**](https://airflow.apache.org)
- Language - [**Python**](https://www.python.org)


